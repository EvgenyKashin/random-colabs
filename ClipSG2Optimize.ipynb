{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i9kpD5m9CQg_",
    "outputId": "52553d61-6a74-4b13-f4fe-45c5bacfe4e7"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/nagolinc/stylegan2-pytorch.git /content/stylegan2-pytorch/\n",
    "%cd /content/stylegan2-pytorch/\n",
    "!git pull\n",
    "!wget https://github.com/EvgenyKashin/random-colabs/releases/download/v0.1/stylegan2-car-config-f.pt -O /content/stylegan2-pytorch/stylegan2-car-config-f.pt\n",
    "!wget https://github.com/EvgenyKashin/random-colabs/releases/download/v0.2/cars_dlatents.pt.zip -O /content/stylegan2-pytorch/cars_dlatents.pt\n",
    "\n",
    "!pip install ninja ftfy regex\n",
    "!wget https://openaipublic.azureedge.net/clip/bpe_simple_vocab_16e6.txt.gz -O bpe_simple_vocab_16e6.txt.gz\n",
    "!wget https://openaipublic.azureedge.net/clip/models/40d365715913c9da98579312b702a82c18be219cc2a73407c4526f58eba950af/ViT-B-32.pt -O model_clip.pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HMY31QfBDSUf",
    "outputId": "c3f7d892-930d-4f6e-fe71-d57204d78b59"
   },
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "CUDA_version = [s for s in subprocess.check_output([\"nvcc\", \"--version\"]).decode(\"UTF-8\").split(\", \") if s.startswith(\"release\")][0].split(\" \")[-1]\n",
    "print(\"CUDA version:\", CUDA_version)\n",
    "\n",
    "if CUDA_version == \"10.0\":\n",
    "    torch_version_suffix = \"+cu100\"\n",
    "elif CUDA_version == \"10.1\":\n",
    "    torch_version_suffix = \"+cu101\"\n",
    "elif CUDA_version == \"10.2\":\n",
    "    torch_version_suffix = \"\"\n",
    "else:\n",
    "    torch_version_suffix = \"+cu110\"\n",
    "  \n",
    "!pip install torch==1.7.1{torch_version_suffix} torchvision==0.8.2{torch_version_suffix} -f https://download.pytorch.org/whl/torch_stable.html\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "print(\"Torch version:\", torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8_FILo1lMxu",
    "outputId": "47742fd4-2d7c-4ad2-8492-bcab6a5f8fa2"
   },
   "outputs": [],
   "source": [
    "%cd /content/stylegan2-pytorch/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZvbaFIECmFg"
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "from argparse import Namespace\n",
    "import math\n",
    "import os\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import lpips\n",
    "\n",
    "from model import Generator, Discriminator\n",
    "\n",
    "\n",
    "def noise_regularize(noises):\n",
    "    loss = 0\n",
    "\n",
    "    for noise in noises:\n",
    "        size = noise.shape[2]\n",
    "\n",
    "        while True:\n",
    "            loss = (\n",
    "                loss\n",
    "                + (noise * torch.roll(noise, shifts=1, dims=3)).mean().pow(2)\n",
    "                + (noise * torch.roll(noise, shifts=1, dims=2)).mean().pow(2)\n",
    "            )\n",
    "\n",
    "            if size <= 8:\n",
    "                break\n",
    "\n",
    "            noise = noise.reshape([-1, 1, size // 2, 2, size // 2, 2])\n",
    "            noise = noise.mean([3, 5])\n",
    "            size //= 2\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "def noise_normalize_(noises):\n",
    "    for noise in noises:\n",
    "        mean = noise.mean()\n",
    "        std = noise.std()\n",
    "\n",
    "        noise.data.add_(-mean).div_(std)\n",
    "\n",
    "\n",
    "def get_lr(t, initial_lr, rampdown=0.25, rampup=0.05):\n",
    "    lr_ramp = min(1, (1 - t) / rampdown)\n",
    "    lr_ramp = 0.5 - 0.5 * math.cos(lr_ramp * math.pi)\n",
    "    lr_ramp = lr_ramp * min(1, t / rampup)\n",
    "\n",
    "    return initial_lr * lr_ramp\n",
    "\n",
    "\n",
    "def latent_noise(latent, strength):\n",
    "    noise = torch.randn_like(latent) * strength\n",
    "\n",
    "    return latent + noise\n",
    "\n",
    "\n",
    "def make_image(tensor):\n",
    "    return (\n",
    "        tensor.detach()\n",
    "        .clamp_(min=-1, max=1)\n",
    "        .add(1)\n",
    "        .div_(2)\n",
    "        .mul(255)\n",
    "        .type(torch.uint8)\n",
    "        .permute(0, 2, 3, 1)\n",
    "        .to(\"cpu\")\n",
    "        .numpy()\n",
    "    )\n",
    "  \n",
    "def prepare_texts(texts):\n",
    "    text_tokens = [tokenizer.encode(\"This is \" + desc) for desc in texts]\n",
    "\n",
    "    text_input = torch.zeros(len(text_tokens), model_clip.context_length, dtype=torch.long)\n",
    "    sot_token = tokenizer.encoder['<|startoftext|>']\n",
    "    eot_token = tokenizer.encoder['<|endoftext|>']\n",
    "\n",
    "    for i, tokens in enumerate(text_tokens):\n",
    "        tokens = [sot_token] + tokens + [eot_token]\n",
    "        text_input[i, :len(tokens)] = torch.tensor(tokens)\n",
    "\n",
    "    text_input = text_input.to(device)\n",
    "    return text_input\n",
    "\n",
    "def image_from_latent(latent):\n",
    "    gen, _ = g_ema([latent], input_is_latent=True, noise=noises)\n",
    "    img = make_image(gen)[0]\n",
    "    return Image.fromarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gCwDWidZWUZl"
   },
   "outputs": [],
   "source": [
    "#@title\n",
    "\n",
    "import gzip\n",
    "import html\n",
    "import os\n",
    "from functools import lru_cache\n",
    "\n",
    "import ftfy\n",
    "import regex as re\n",
    "\n",
    "\n",
    "@lru_cache()\n",
    "def bytes_to_unicode():\n",
    "    \"\"\"\n",
    "    Returns list of utf-8 byte and a corresponding list of unicode strings.\n",
    "    The reversible bpe codes work on unicode strings.\n",
    "    This means you need a large # of unicode characters in your vocab if you want to avoid UNKs.\n",
    "    When you're at something like a 10B token dataset you end up needing around 5K for decent coverage.\n",
    "    This is a signficant percentage of your normal, say, 32K bpe vocab.\n",
    "    To avoid that, we want lookup tables between utf-8 bytes and unicode strings.\n",
    "    And avoids mapping to whitespace/control characters the bpe code barfs on.\n",
    "    \"\"\"\n",
    "    bs = list(range(ord(\"!\"), ord(\"~\")+1))+list(range(ord(\"¡\"), ord(\"¬\")+1))+list(range(ord(\"®\"), ord(\"ÿ\")+1))\n",
    "    cs = bs[:]\n",
    "    n = 0\n",
    "    for b in range(2**8):\n",
    "        if b not in bs:\n",
    "            bs.append(b)\n",
    "            cs.append(2**8+n)\n",
    "            n += 1\n",
    "    cs = [chr(n) for n in cs]\n",
    "    return dict(zip(bs, cs))\n",
    "\n",
    "\n",
    "def get_pairs(word):\n",
    "    \"\"\"Return set of symbol pairs in a word.\n",
    "    Word is represented as tuple of symbols (symbols being variable-length strings).\n",
    "    \"\"\"\n",
    "    pairs = set()\n",
    "    prev_char = word[0]\n",
    "    for char in word[1:]:\n",
    "        pairs.add((prev_char, char))\n",
    "        prev_char = char\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def basic_clean(text):\n",
    "    text = ftfy.fix_text(text)\n",
    "    text = html.unescape(html.unescape(text))\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def whitespace_clean(text):\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "\n",
    "class SimpleTokenizer(object):\n",
    "    def __init__(self, bpe_path: str = \"bpe_simple_vocab_16e6.txt.gz\"):\n",
    "        self.byte_encoder = bytes_to_unicode()\n",
    "        self.byte_decoder = {v: k for k, v in self.byte_encoder.items()}\n",
    "        merges = gzip.open(bpe_path).read().decode(\"utf-8\").split('\\n')\n",
    "        merges = merges[1:49152-256-2+1]\n",
    "        merges = [tuple(merge.split()) for merge in merges]\n",
    "        vocab = list(bytes_to_unicode().values())\n",
    "        vocab = vocab + [v+'</w>' for v in vocab]\n",
    "        for merge in merges:\n",
    "            vocab.append(''.join(merge))\n",
    "        vocab.extend(['<|startoftext|>', '<|endoftext|>'])\n",
    "        self.encoder = dict(zip(vocab, range(len(vocab))))\n",
    "        self.decoder = {v: k for k, v in self.encoder.items()}\n",
    "        self.bpe_ranks = dict(zip(merges, range(len(merges))))\n",
    "        self.cache = {'<|startoftext|>': '<|startoftext|>', '<|endoftext|>': '<|endoftext|>'}\n",
    "        self.pat = re.compile(r\"\"\"<\\|startoftext\\|>|<\\|endoftext\\|>|'s|'t|'re|'ve|'m|'ll|'d|[\\p{L}]+|[\\p{N}]|[^\\s\\p{L}\\p{N}]+\"\"\", re.IGNORECASE)\n",
    "\n",
    "    def bpe(self, token):\n",
    "        if token in self.cache:\n",
    "            return self.cache[token]\n",
    "        word = tuple(token[:-1]) + ( token[-1] + '</w>',)\n",
    "        pairs = get_pairs(word)\n",
    "\n",
    "        if not pairs:\n",
    "            return token+'</w>'\n",
    "\n",
    "        while True:\n",
    "            bigram = min(pairs, key = lambda pair: self.bpe_ranks.get(pair, float('inf')))\n",
    "            if bigram not in self.bpe_ranks:\n",
    "                break\n",
    "            first, second = bigram\n",
    "            new_word = []\n",
    "            i = 0\n",
    "            while i < len(word):\n",
    "                try:\n",
    "                    j = word.index(first, i)\n",
    "                    new_word.extend(word[i:j])\n",
    "                    i = j\n",
    "                except:\n",
    "                    new_word.extend(word[i:])\n",
    "                    break\n",
    "\n",
    "                if word[i] == first and i < len(word)-1 and word[i+1] == second:\n",
    "                    new_word.append(first+second)\n",
    "                    i += 2\n",
    "                else:\n",
    "                    new_word.append(word[i])\n",
    "                    i += 1\n",
    "            new_word = tuple(new_word)\n",
    "            word = new_word\n",
    "            if len(word) == 1:\n",
    "                break\n",
    "            else:\n",
    "                pairs = get_pairs(word)\n",
    "        word = ' '.join(word)\n",
    "        self.cache[token] = word\n",
    "        return word\n",
    "\n",
    "    def encode(self, text):\n",
    "        bpe_tokens = []\n",
    "        text = whitespace_clean(basic_clean(text)).lower()\n",
    "        for token in re.findall(self.pat, text):\n",
    "            token = ''.join(self.byte_encoder[b] for b in token.encode('utf-8'))\n",
    "            bpe_tokens.extend(self.encoder[bpe_token] for bpe_token in self.bpe(token).split(' '))\n",
    "        return bpe_tokens\n",
    "\n",
    "    def decode(self, tokens):\n",
    "        text = ''.join([self.decoder[token] for token in tokens])\n",
    "        text = bytearray([self.byte_decoder[c] for c in text]).decode('utf-8', errors=\"replace\").replace('</w>', ' ')\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9VQi10Cybm4_"
   },
   "source": [
    "## Init CLIP and SG2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 151,
     "referenced_widgets": [
      "e17283f256bb46d898843761d0b51393",
      "ddf1be1cebf844ff92a7a068bf6e0a2b",
      "ea7e4cc0bf4746129c0eccaaaa28fd09",
      "27e937e0a29a4d5f806dea37e46ec881",
      "3a96d8e6fd5d4eff805996db77d41c12",
      "67256dc7f25e4ff2a8a1f7ab86586dbc",
      "b673ae53684c4310a0131ef8c8235a51",
      "44842cc15299474ca586fab332afbb63"
     ]
    },
    "id": "oUCM3e_oCmBQ",
    "outputId": "17534dcf-b55f-4f93-dd8b-2e33bb97f876"
   },
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    ckpt='/content/stylegan2-pytorch/stylegan2-car-config-f.pt',\n",
    "    size=512,\n",
    "    lr_rampup=0.05,\n",
    "    lr_rampdown=0.25,\n",
    "    noise=0.0,\n",
    "    noise_ramp=0.75,\n",
    "    noise_regularize=0,\n",
    "    w_plus=False,\n",
    "    step=200,\n",
    "    lr=0.02,\n",
    "    clip_weight=1,\n",
    "    lpips_weight=0,\n",
    "    l2_weight=0,\n",
    "    latent_reg_weight=0,\n",
    "    disc_weight=0,\n",
    "    truncation=0.8\n",
    ")\n",
    "\n",
    "device = \"cuda\"\n",
    "model_clip = torch.jit.load(\"model_clip.pt\").to(device).eval()\n",
    "\n",
    "n_mean_latent = 10000\n",
    "resize = min(args.size, 256)\n",
    "resize_clip = model_clip.input_resolution.item()\n",
    "\n",
    "\n",
    "tokenizer = SimpleTokenizer()\n",
    "\n",
    "transform_gen = Compose(\n",
    "    [\n",
    "        Resize(resize),\n",
    "        CenterCrop(resize),\n",
    "        ToTensor(),\n",
    "        Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "transform_clip = Compose([\n",
    "    Resize(resize_clip, interpolation=Image.BICUBIC),\n",
    "    CenterCrop(resize_clip),\n",
    "    ToTensor(),\n",
    "    Normalize([0.48145466, 0.4578275, 0.40821073],\n",
    "              [0.26862954, 0.26130258, 0.27577711])\n",
    "])\n",
    "\n",
    "class UnNormalize(object):\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tensor (Tensor): Tensor image of size (C, H, W) to be normalized.\n",
    "        Returns:\n",
    "            Tensor: Normalized image.\n",
    "        \"\"\"\n",
    "        # The normalize code -> t.sub_(m).div_(s)\n",
    "        tensor = tensor * self.std[:, None, None]\n",
    "        tensor = tensor + self.mean[:, None, None]\n",
    "\n",
    "        return tensor.clamp(0.0, 1.0)\n",
    "\n",
    "\n",
    "transform_clip_after_gen = Compose([\n",
    "    Resize(resize_clip, interpolation=Image.BICUBIC),\n",
    "    CenterCrop(resize_clip),\n",
    "    UnNormalize(torch.tensor([0.5, 0.5, 0.5]).cuda(),\n",
    "                torch.tensor([0.5, 0.5, 0.5]).cuda()),\n",
    "    Normalize([0.48145466, 0.4578275, 0.40821073],\n",
    "              [0.26862954, 0.26130258, 0.27577711])\n",
    "])\n",
    "\n",
    "\n",
    "def clip_similarity_score(img, text_features):\n",
    "    image_features = model_clip.encode_image(img).float()\n",
    "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "    similarity = text_features @ image_features.T\n",
    "    return similarity.mean()\n",
    "\n",
    "\n",
    "checkpoint_sg2 = torch.load(args.ckpt)\n",
    "g_ema = Generator(args.size, 512, 8)\n",
    "g_ema.load_state_dict(checkpoint_sg2[\"g_ema\"], strict=True)\n",
    "g_ema = g_ema.to(device).eval()\n",
    "\n",
    "disc = Discriminator(args.size)\n",
    "disc.load_state_dict(checkpoint_sg2['d'], strict=True)\n",
    "disc.to(device).eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    noise_sample = torch.randn(n_mean_latent, 512, device=device)\n",
    "    latent_out = g_ema.style(noise_sample)\n",
    "\n",
    "    latent_mean = latent_out.mean(0)\n",
    "    latent_std = ((latent_out - latent_mean).pow(2).sum() / n_mean_latent) ** 0.5\n",
    "\n",
    "percept = lpips.PerceptualLoss(\n",
    "        model=\"net-lin\", net=\"vgg\", use_gpu=device.startswith(\"cuda\")\n",
    "    )\n",
    "\n",
    "dlatents = torch.load('/content/stylegan2-pytorch/cars_dlatents.pt')\n",
    "keys = ['cars_crop/5_3.jpg', 'cars_crop/3_4.jpg', 'cars_crop/8_1.jpg', 'cars_crop/9_2.jpg']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g63zosllb0QR"
   },
   "source": [
    "## Optimization loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A4MXqqlfJR5o"
   },
   "outputs": [],
   "source": [
    "def optimize_latent_to_text(latent, text):\n",
    "    noises_single = g_ema.make_noise()\n",
    "    noises = []\n",
    "    n_samples = len(text)\n",
    "    for noise in noises_single:\n",
    "        noises.append(noise.repeat(n_samples, 1, 1, 1).normal_())\n",
    "      \n",
    "    texts = prepare_texts(text)\n",
    "    text_features = model_clip.encode_text(texts).detach().float()\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "\n",
    "    latent_in = latent.clone()\n",
    "    latent_init = latent_in.clone()\n",
    "    img_gen_init, _ = g_ema([latent_init], input_is_latent=True, noise=noises,\n",
    "                            truncation=args.truncation, truncation_latent=latent_mean)\n",
    "    img_gen_init = img_gen_init.detach().clone()\n",
    "\n",
    "    if args.w_plus:\n",
    "        latent_in = latent_in.unsqueeze(1).repeat(1, g_ema.n_latent, 1)\n",
    "\n",
    "    latent_in.requires_grad = True\n",
    "\n",
    "    if args.noise_regularize != 0:\n",
    "        for noise in noises:\n",
    "            noise.requires_grad = True\n",
    "\n",
    "    params = [latent_in]\n",
    "    if args.noise_regularize != 0:\n",
    "        params += noises\n",
    "\n",
    "    # optimizer = optim.SGD(params, lr=args.lr, momentum=0.9)\n",
    "    optimizer = optim.Adam(params, lr=args.lr)\n",
    "\n",
    "    pbar = tqdm(range(args.step))\n",
    "    latent_path = []\n",
    "\n",
    "    losses = defaultdict(list)\n",
    "\n",
    "    for i in pbar:\n",
    "        t = i / args.step\n",
    "        lr = get_lr(t, args.lr)\n",
    "        optimizer.param_groups[0][\"lr\"] = lr\n",
    "        noise_strength = latent_std * args.noise * max(0, 1 - t / args.noise_ramp) ** 2\n",
    "        latent_n = latent_noise(latent_in, noise_strength.item())\n",
    "\n",
    "        img_gen, _ = g_ema([latent_n], input_is_latent=True, noise=noises,\n",
    "                          truncation=args.truncation, truncation_latent=latent_mean)\n",
    "\n",
    "        batch, channel, height, width = img_gen.shape\n",
    "\n",
    "        # if height > 256:\n",
    "        #     factor = height // 256\n",
    "\n",
    "        #     img_gen = img_gen.reshape(\n",
    "        #         batch, channel, height // factor, factor, width // factor, factor\n",
    "        #     )\n",
    "        #     img_gen = img_gen.mean([3, 5])\n",
    "\n",
    "        lpips_loss = percept(img_gen, img_gen_init).sum()\n",
    "        mse_loss = F.mse_loss(img_gen, img_gen_init)\n",
    "        n_loss = noise_regularize(noises)\n",
    "        latent_loss = F.mse_loss(latent_init, latent_n)\n",
    "        disc_score = disc(img_gen)\n",
    "\n",
    "        img_gen = transform_clip_after_gen(img_gen)\n",
    "        clip_score = clip_similarity_score(img_gen, text_features)\n",
    "\n",
    "        loss = -clip_score * args.clip_weight + lpips_loss * args.lpips_weight \\\n",
    "              + mse_loss * args.l2_weight + n_loss * args.noise_regularize \\\n",
    "              + latent_loss * args.latent_reg_weight - disc_score * args.disc_weight\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        noise_normalize_(noises)\n",
    "\n",
    "        \n",
    "        losses['loss'].append(loss.item())\n",
    "        losses['loss_grad'].append(latent_in.grad.norm().item())\n",
    "        losses['clip_score'].append(clip_score.item())\n",
    "        losses['lpips_loss'].append(lpips_loss.item())\n",
    "        losses['latent_loss'].append(latent_loss.item())\n",
    "        losses['disc_score'].append(disc_score.item())\n",
    "\n",
    "        if (i + 1) % 10 == 0:\n",
    "            latent_path.append(latent_in.detach().clone())\n",
    "\n",
    "        pbar.set_description(\n",
    "            (   f\"Loss: {loss.item():.4f}; Loss gr: {latent_in.grad.norm().item():.4f}; \"\n",
    "                f\"latent diff: {latent_loss.item():.4f}; \"\n",
    "              # f\"D score: {disc_score.item():.4f}; \"\n",
    "                f\"lpips: {lpips_loss.item():.4f}; \"\n",
    "            ), refresh=True\n",
    "        )\n",
    "\n",
    "    return latent_path, losses, noises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G50P2w4jb8zR"
   },
   "source": [
    "### The most important params for tweaking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LKUMR_6iVm7u"
   },
   "outputs": [],
   "source": [
    "args.lpips_weight = 0 # 1e-1\n",
    "args.latent_reg_weight = 0 #1e-1\n",
    "args.noise = 0.0\n",
    "args.noise_regularize = 0\n",
    "args.disc_weight = 0\n",
    "args.clip_weight = 1\n",
    "args.lr = 0.04\n",
    "args.step = 150\n",
    "args.truncation = 0.8\n",
    "args.disc_weight = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "be8VlCiJrZpO"
   },
   "outputs": [],
   "source": [
    "descriptions = [\n",
    "    'light weight rear wheel drive coupe highly tuned Japanese automobile reflecting the original styles of drifting with a big spoiler',\n",
    "    'white car standing in front of the camera ',\n",
    "    'modified big jeep car for racing',\n",
    "    'car standing in front of the mountains',\n",
    "    'car turned forward',\n",
    "    'a car standing in front of the ocean on the sand',\n",
    "    'nissan car',  # toyota Mercedes ferrari\n",
    "    'black and white photo of the car'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bkkhS5SLeUM"
   },
   "outputs": [],
   "source": [
    "latent_in_mean = latent_mean.detach().clone().unsqueeze(0)  #  .repeat(1, 1)\n",
    "# 1 3 4 5\n",
    "torch.manual_seed(4)\n",
    "latent_z = torch.randn(1, 512, device=device)\n",
    "latent_in_seed = g_ema.style(latent_z).detach()  # .repeat(1, 1)\n",
    "\n",
    "latent_in_car = torch.unsqueeze(dlatents[keys[0]]['latent'], 0).detach().clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7Ryef4SCLbXr",
    "outputId": "c1b6c112-0fc4-4b5c-fbdb-d1e401f58735"
   },
   "outputs": [],
   "source": [
    "latent_path, losses, noises = optimize_latent_to_text(latent_in_car, [descriptions[6]])  # latent_in_car or latent_in_seed or latent_in_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "WnTRM_ppOGuR",
    "outputId": "4fe342c9-e037-4c6e-c4a1-07f3b7569a9b"
   },
   "outputs": [],
   "source": [
    "image_from_latent(latent_path[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "id": "GvBB-NMsNbnK",
    "outputId": "83b5db94-1d06-41ea-c6c6-e2050f72a3e6"
   },
   "outputs": [],
   "source": [
    "image_from_latent(latent_path[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "XIxCAChb8U9Y",
    "outputId": "dba23a28-f2ad-4942-cd10-db7f58c37a24"
   },
   "outputs": [],
   "source": [
    "for k, v in losses.items():\n",
    "    plt.figure()\n",
    "    plt.title(k)\n",
    "    plt.plot(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "o5ByabVpn9DQ",
    "outputId": "a9195cd9-4b03-4371-8869-9212e91b8ef0"
   },
   "outputs": [],
   "source": [
    "path_ar = []\n",
    "for i, latent in enumerate(latent_path):\n",
    "    if i % 1 == 0:\n",
    "        display(image_from_latent(latent))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hysBp3rCUp8y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "ClipCarOptimizev2",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "27e937e0a29a4d5f806dea37e46ec881": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_44842cc15299474ca586fab332afbb63",
      "placeholder": "​",
      "style": "IPY_MODEL_b673ae53684c4310a0131ef8c8235a51",
      "value": " 528M/528M [00:04&lt;00:00, 122MB/s]"
     }
    },
    "3a96d8e6fd5d4eff805996db77d41c12": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "44842cc15299474ca586fab332afbb63": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "67256dc7f25e4ff2a8a1f7ab86586dbc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b673ae53684c4310a0131ef8c8235a51": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ddf1be1cebf844ff92a7a068bf6e0a2b": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e17283f256bb46d898843761d0b51393": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_ea7e4cc0bf4746129c0eccaaaa28fd09",
       "IPY_MODEL_27e937e0a29a4d5f806dea37e46ec881"
      ],
      "layout": "IPY_MODEL_ddf1be1cebf844ff92a7a068bf6e0a2b"
     }
    },
    "ea7e4cc0bf4746129c0eccaaaa28fd09": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_67256dc7f25e4ff2a8a1f7ab86586dbc",
      "max": 553433881,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_3a96d8e6fd5d4eff805996db77d41c12",
      "value": 553433881
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
